{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chainerで線形回帰してみる\n",
    "\n",
    "線形回帰についてはtensorflowを参考。ここではChainer独自のところを説明していく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, Variable, optimizers, serializers, utils\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChainerではVariableを使う。Chainerでは、入力の次元の配列を作らないといけないことに注意する。一次元の入力であれば、`[[data1],[data2],...]`というVariableを使う。(cf. Tensorflowでは`[data1, data2, ...]`であった。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<var@110ce3c50>\n",
      "x.data -> [ 2.]\n",
      "x.grad -> None\n",
      "y.data-> [ 4.]\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([2], dtype=np.float32)\n",
    "x = Variable(x_data)\n",
    "print(x)\n",
    "print(\"x.data ->\", x.data)\n",
    "print(\"x.grad ->\", x.grad) # この段階でgradはNone\n",
    "y = x ** 2\n",
    "print(\"y.data->\", y.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アウトプットに対して、`backward()`とすると、その関数を作っているVariableでのgradの値が入力される。この場合はxでの偏微分の値。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.]\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(x.grad)\n",
    "y.zerograd() # 偏微分の値は蓄積されていくので、zerogradで初期化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chainerが面白いのは、F.LinerやL.Linerが\n",
    "$$ \\vec{y} = W\\vec{x} +\\vec{b}$$\n",
    "の形を持った関数で、$W$や$\\vec{b}$はランダムに初期化してくれている。\n",
    "\n",
    "`F.Liner(1,1)`だと、1次元→1次元で、あとでMNISTなどで使う784次元→10次元であれば`F.Liner(784,10)`とすれば、&W&は784x10の行列で、$\\vec{b}\\in\\mathbb{R}^{10}$となるように作ってくれる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.07353922]]\n",
      "[ 0.]\n"
     ]
    }
   ],
   "source": [
    "f = F.Linear(1,1) # 1次元→1次元の y = Wx + b\n",
    "print(f.W.data)\n",
    "print(f.b.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100個のデータセットを作って、コスト関数を平均二乗誤差関数で定義してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.15548615157604218, dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = np.random.rand(100,1).astype(\"float32\")\n",
    "y_data = x_data * 0.1 + 0.3\n",
    "x = Variable(x_data)\n",
    "y = Variable(y_data)\n",
    "loss = F.mean_squared_error(f(x), y) # 平均二乗誤差\n",
    "loss.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチで一気に計算すると器用に偏微分の値は蓄積されていくみたいなので、偏微分を計算する前に初期化しておくための`zerograds`が準備されているので呼ぶ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.44013578]]\n",
      "[-0.78197145]\n"
     ]
    }
   ],
   "source": [
    "f.zerograds() # 微分値の初期化\n",
    "loss.backward() # それぞれの偏微分を計算する\n",
    "print(f.W.grad) # fのWでの変微分の値が求まる\n",
    "print(f.b.grad) # fのbでの変微分の値が求まる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コスト関数の最小化の手法はoptimizersに準備されている。最急降下法がなさそうなので、確率的最急降下法を使う。`setup`で変数の更新対象の関数をセットする。そして、`update`でパラメーターを更新してくれる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.14652866]]\n",
      "[ 0.39098573]\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.SGD(0.5) # 確率的最急降下法\n",
    "optimizer.setup(f)\n",
    "optimizer.update()\n",
    "print(f.W.data)\n",
    "print(f.b.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記を合わせてイテレーションして学習を進めてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[ 0.08199314]] [ 0.27560526]\n",
      "20 [[ 0.09918599]] [ 0.30045748]\n",
      "40 [[ 0.09979708]] [ 0.30011407]\n",
      "60 [[ 0.09994941]] [ 0.30002844]\n",
      "80 [[ 0.09998742]] [ 0.30000708]\n",
      "100 [[ 0.09999687]] [ 0.30000177]\n",
      "120 [[ 0.09999921]] [ 0.30000046]\n",
      "140 [[ 0.0999998]] [ 0.30000013]\n",
      "160 [[ 0.0999999]] [ 0.30000007]\n",
      "180 [[ 0.0999999]] [ 0.30000007]\n",
      "200 [[ 0.0999999]] [ 0.30000007]\n"
     ]
    }
   ],
   "source": [
    "for step in range(201):\n",
    "    loss = F.mean_squared_error(f(x), y)  # 再度コスト関数を計算する\n",
    "    f.zerograds() # gradは保存されていくので初期化する\n",
    "    loss.backward() # 微分を計算\n",
    "    optimizer.update() # 確率的最急降下法で fをupdateする\n",
    "    if step % 20 == 0:\n",
    "        print(step,f.W.data, f.b.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "綺麗に、学習されている。\n",
    "\n",
    "次に、Chainというクラスを使うことにより、もう少し便利に、上記が出来ることを見ていく。深層学習などの複雑なニューラルネットワークを生成するには、こちらを使うほうが便利そうだ。やっていることは上記と同じなので、説明は省く。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LinearRegression(Chain):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__(\n",
    "            l1 = L.Linear(1,1)\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x, y):\n",
    "        return F.mean_squared_error(self.l1(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "optimizer = optimizers.SGD(0.5)\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data ->  [[ 0.21073872]\n",
      " [ 0.36733896]\n",
      " [ 0.54460317]\n",
      " [ 0.90479493]\n",
      " [ 0.77130264]]\n",
      "y_data ->  [[ 0.32107389]\n",
      " [ 0.33673391]\n",
      " [ 0.35446033]\n",
      " [ 0.39047951]\n",
      " [ 0.37713027]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_data = np.random.rand(100,1).astype(\"float32\")\n",
    "y_data = x_data * 0.1 + 0.3\n",
    "print(\"x_data -> \", x_data[:5])\n",
    "print(\"y_data -> \", y_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[ 1.33077836]] [-0.72787428]\n",
      "20 [[ 0.44749212]] [ 0.08820534]\n",
      "40 [[ 0.18916766]] [ 0.24565278]\n",
      "60 [[ 0.12288073]] [ 0.28605431]\n",
      "80 [[ 0.10587127]] [ 0.2964215]\n",
      "100 [[ 0.10150659]] [ 0.29908174]\n",
      "120 [[ 0.10038661]] [ 0.29976436]\n",
      "140 [[ 0.10009921]] [ 0.29993954]\n",
      "160 [[ 0.10002545]] [ 0.29998451]\n",
      "180 [[ 0.10000654]] [ 0.29999602]\n",
      "200 [[ 0.10000168]] [ 0.299999]\n"
     ]
    }
   ],
   "source": [
    "for step in range(201):\n",
    "    train_x = Variable(x_data)\n",
    "    train_y = Variable(y_data)\n",
    "    optimizer.update(model, train_x, train_y)\n",
    "    if step % 20 == 0:\n",
    "        print(step, model.l1.W.data, model.l1.b.data) \n",
    "    \n",
    "#     model.zerograds()\n",
    "#     loss = model(train_x, train_y)\n",
    "#     loss.backword()\n",
    "#     optimizer.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "綺麗に学習できている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
