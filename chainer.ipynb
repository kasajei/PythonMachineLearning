{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chainerで線形回帰してみる\n",
    "\n",
    "線形回帰についてはtensorflowを参考。ここではChainer独自のところを説明していく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, Variable, optimizers, serializers, utils\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChainerでもVariableを使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<var@10e80f518>\n",
      "x.data -> [ 2.]\n",
      "x.grad -> None\n",
      "y.data-> [ 4.]\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([2], dtype=np.float32)\n",
    "x = Variable(x_data)\n",
    "print(x)\n",
    "print(\"x.data ->\", x.data)\n",
    "print(\"x.grad ->\", x.grad) # この段階でgradはNone\n",
    "y = x ** 2\n",
    "print(\"y.data->\", y.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アウトプットに対して、`backward()`とすると、その関数を作っているVariableでのgradの値が入力される。この場合はxでの偏微分の値。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.]\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(x.grad)\n",
    "y.zerograd() # 偏微分の値は蓄積されていくので、zerogradで初期化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chainerが面白いのは、F.LinerやL.Linerが\n",
    "$$ \\vec{y} = W\\vec{x} +\\vec{b}$$\n",
    "の形を持った関数で、$W$や$\\vec{b}$はランダムに初期化してくれている。\n",
    "\n",
    "`F.Liner(1,1)`だと、1次元→1次元で、あとでMNISTなどで使う784次元→10次元であれば`F.Liner(784,10)`とすれば、&W&は784x10の行列で、$\\vec{b}\\in\\mathbb{R}^{10}$となるように作ってくれる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.70886648]]\n",
      "[ 0.]\n"
     ]
    }
   ],
   "source": [
    "f = F.Linear(1,1) # 1次元→1次元の y = Wx + b\n",
    "print(f.W.data)\n",
    "print(f.b.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100個のデータセットを作って、コスト関数を平均二乗誤差関数で定義してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1.7175734043121338, dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = np.random.rand(100,1).astype(\"float32\")\n",
    "y_data = x_data * 0.1 + 0.3\n",
    "x = Variable(x_data)\n",
    "y = Variable(y_data)\n",
    "loss = F.mean_squared_error(f(x), y) # 平均二乗誤差\n",
    "loss.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチで一気に計算すると器用に偏微分の値は蓄積されていくみたいなので、偏微分を計算する前に初期化しておくための`zerograds`が準備されているので呼ぶ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.50500262]]\n",
      "[-2.37599277]\n"
     ]
    }
   ],
   "source": [
    "f.zerograds() # 微分値の初期化\n",
    "loss.backward() # それぞれの偏微分を計算する\n",
    "print(f.W.grad) # fのWでの変微分の値が求まる\n",
    "print(f.b.grad) # fのbでの変微分の値が求まる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コスト関数の最小化の手法はoptimizersに準備されている。最急降下法がなさそうなので、確率的最急降下法を使う。`setup`で変数の更新対象の関数をセットする。そして、`update`でパラメーターを更新してくれる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.95636517]]\n",
      "[ 1.18799639]\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.SGD(0.5) # 確率的最急降下法\n",
    "optimizer.setup(f)\n",
    "optimizer.update()\n",
    "print(f.W.data)\n",
    "print(f.b.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記を合わせてイテレーションして学習を進めてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[-1.03884578]] [ 0.81858361]\n",
      "20 [[-0.13580334]] [ 0.42504498]\n",
      "40 [[ 0.04961303]] [ 0.32671988]\n",
      "60 [[ 0.08923323]] [ 0.30570957]\n",
      "80 [[ 0.09769934]] [ 0.30122006]\n",
      "100 [[ 0.0995084]] [ 0.30026069]\n",
      "120 [[ 0.09989496]] [ 0.30005571]\n",
      "140 [[ 0.09997758]] [ 0.3000119]\n",
      "160 [[ 0.09999523]] [ 0.30000255]\n",
      "180 [[ 0.09999897]] [ 0.30000055]\n",
      "200 [[ 0.09999976]] [ 0.30000013]\n"
     ]
    }
   ],
   "source": [
    "for step in range(201):\n",
    "    loss = F.mean_squared_error(f(x), y)  # 再度コスト関数を計算する\n",
    "    f.zerograds() # gradは保存されていくので初期化する\n",
    "    loss.backward() # 微分を計算\n",
    "    optimizer.update() # 確率的勾配降下法で fをupdateする\n",
    "    if step % 20 == 0:\n",
    "        print(step,f.W.data, f.b.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "綺麗に、学習されている。\n",
    "\n",
    "次に、Chainというクラスを使うことにより、もう少し便利に、上記が出来ることを見ていく。深層学習などの複雑なニューラルネットワークを生成するには、こちらを使うほうが便利そうだ。やっていることは上記と同じなので、説明は省く。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LinearRegression(Chain):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__(\n",
    "            l1 = L.Linear(1,1)\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x, y):\n",
    "        return F.mean_squared_error(self.l1(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "optimizer = optimizers.SGD(0.5)\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data ->  [[ 0.69146651]\n",
      " [ 0.01516824]\n",
      " [ 0.27602309]\n",
      " [ 0.78357798]\n",
      " [ 0.84887975]]\n",
      "y_data ->  [[ 0.36914667]\n",
      " [ 0.30151683]\n",
      " [ 0.32760233]\n",
      " [ 0.37835783]\n",
      " [ 0.38488799]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_data = np.random.rand(100,1).astype(\"float32\")\n",
    "y_data = x_data * 0.1 + 0.3\n",
    "print(\"x_data -> \", x_data[:5])\n",
    "print(\"y_data -> \", y_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[ 0.77943921]] [-0.13977495]\n",
      "20 [[ 0.28248915]] [ 0.19582576]\n",
      "40 [[ 0.14745325]] [ 0.27291125]\n",
      "60 [[ 0.11233943]] [ 0.29295602]\n",
      "80 [[ 0.10320868]] [ 0.29816833]\n",
      "100 [[ 0.10083436]] [ 0.29952371]\n",
      "120 [[ 0.10021696]] [ 0.29987615]\n",
      "140 [[ 0.10005641]] [ 0.29996783]\n",
      "160 [[ 0.10001469]] [ 0.29999164]\n",
      "180 [[ 0.10000382]] [ 0.29999784]\n",
      "200 [[ 0.10000101]] [ 0.29999945]\n"
     ]
    }
   ],
   "source": [
    "for step in range(201):\n",
    "    train_x = Variable(x_data)\n",
    "    train_y = Variable(y_data)\n",
    "    optimizer.update(model, train_x, train_y)\n",
    "    if step % 20 == 0:\n",
    "        print(step, model.l1.W.data, model.l1.b.data) \n",
    "    \n",
    "#     model.zerograds()\n",
    "#     loss = model(train_x, train_y)\n",
    "#     loss.backword()\n",
    "#     optimizer.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "綺麗に学習できている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MNISTをパーセプトロンで学習してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000 10000\n",
      "784\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "28x28の画像のモノクロ(白0→黒1)が１次元配列で入っている [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "最初のラベルは 5\n",
      "[5 0 4 ..., 8 4 8]\n"
     ]
    }
   ],
   "source": [
    "import pickle, gzip\n",
    "f = gzip.open('data/mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "print(len(train_set[0]), len(valid_set[0]), len(test_set[0]))\n",
    "train_set_x, train_set_y  = train_set\n",
    "test_set_x, test_set_y = test_set\n",
    "print(len(train_set_x[0]))\n",
    "print(train_set_x)\n",
    "print(\"28x28の画像のモノクロ(白0→黒1)が１次元配列で入っている\",train_set_x[:5])\n",
    "print(\"最初のラベルは\",train_set_y[0])\n",
    "\n",
    "# あとで、softmax_cross_entropyを使うときに型の判定があり、np.int32じゃないといけない\n",
    "train_set_y = train_set_y.astype(np.int32)\n",
    "test_set_y = test_set_y.astype(np.int32)\n",
    "print(train_set_y )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MNISTPerseptron(Chain):\n",
    "    def __init__(self):\n",
    "        super(MNISTPerseptron, self).__init__(\n",
    "            l1 = L.Linear(784, 10)\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x, y):\n",
    "        pridict = self.l1(x)\n",
    "        self.loss = F.softmax_cross_entropy(pridict, y)\n",
    "        self.accuracy = F.accuracy(pridict, y) \n",
    "        return self.loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = MNISTPerseptron()\n",
    "optimizer = optimizers.SGD(0.1) # 確率的勾配降下法なので、ちょっと大きめに\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "for step in range(1000):\n",
    "    batch_index = np.random.randint(len(train_set[0])-batch_size)\n",
    "    batch_x = Variable(train_set_x[batch_index:batch_index+batch_size])\n",
    "    batch_y = Variable(train_set_y[batch_index:batch_index+batch_size])\n",
    "    optimizer.update(model, batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.910099983215332\n"
     ]
    }
   ],
   "source": [
    "model(Variable(test_set_x),Variable(test_set_y))\n",
    "print(model.accuracy.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90%ぐらいの制度がでる"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
