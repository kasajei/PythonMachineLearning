{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python が 大好き です \n",
      "\n",
      "無理 だ ー \n",
      "\n",
      "人間 よ 、 24 に 、 分類 さ れよ ！ \n",
      "\n",
      "計算 中 ・ ・ ・ \n",
      "\n",
      "えい 、 ` from scipy . cluster . vq import vq , kmeans , whiten ` \n",
      "\n",
      "k - means やる ぞ \n",
      "\n",
      "お は よー ！ 6 h 28 m ね た よ ！ January 27 , 2016 at 01 : 19 AM → January 27 , 2016 at 08 : 06 AM \n",
      "\n",
      "ねむい ー \n",
      "\n",
      "Doc 2 Vec の 基底 ベクトル は Word 2 Vec の それ と 一緒 っぽい な \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import MeCab\n",
    "from gensim import models\n",
    "\n",
    "df = pd.read_csv(\"../data/tweets.csv\")\n",
    "mecab = MeCab.Tagger (\"-Owakati\")\n",
    "print(mecab.parse(\"pythonが大好きです\"))\n",
    "\n",
    "df = df[pd.isnull(df.retweeted_status_id)]\n",
    "df = df[pd.isnull(df.in_reply_to_status_id)]\n",
    "df = df[df.text.str.find(\"http\")==-1]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if not pd.isnull(row.text):\n",
    "        df.set_value(index, \"text\", mecab.parse(row[\"text\"]))        \n",
    "        \n",
    "for index, row in df.iterrows():\n",
    "    if not pd.isnull(row.text):\n",
    "        print(row.text)\n",
    "    if index > 10 : break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/formatted_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledSentence(words=['無理', 'だ', 'ー'], tags=['text_1']), LabeledSentence(words=['人間', 'よ', '、', '24', 'に', '、', '分類', 'さ', 'れよ', '！'], tags=['text_2']), LabeledSentence(words=['計算', '中', '・', '・', '・'], tags=['text_3']), LabeledSentence(words=['えい', '、', '`', 'from', 'scipy', '.', 'cluster', '.', 'vq', 'import', 'vq', ',', 'kmeans', ',', 'whiten', '`'], tags=['text_4']), LabeledSentence(words=['k', '-', 'means', 'やる', 'ぞ'], tags=['text_5'])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8423"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __iter__(self):\n",
    "        for index, row in self.df.iterrows():            \n",
    "            yield models.doc2vec.LabeledSentence(row[\"text\"].split(), ['text_%s' % index])\n",
    "            \n",
    "\n",
    "sentence = LabeledLineSentence(df)\n",
    "print(list(sentence)[:5])\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "トレーニング0終了\n",
      "トレーニング1終了\n",
      "トレーニング2終了\n",
      "トレーニング3終了\n",
      "トレーニング4終了\n",
      "トレーニング5終了\n",
      "トレーニング6終了\n",
      "トレーニング7終了\n",
      "トレーニング8終了\n",
      "トレーニング9終了\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "model = models.Doc2Vec()\n",
    "model.build_vocab(sentence)\n",
    "\n",
    "for _ in range(10):\n",
    "    model.train(sentence)\n",
    "    print(\"トレーニング\" + str(_) + \"終了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('普通', 0.6398686170578003),\n",
       " ('波', 0.636785626411438),\n",
       " ('iCloud', 0.6355605721473694),\n",
       " ('秋', 0.6327695846557617),\n",
       " ('違い', 0.6275097131729126),\n",
       " ('標準', 0.61744225025177),\n",
       " ('最後', 0.6063286066055298),\n",
       " ('スクショ', 0.599561333656311),\n",
       " ('うち', 0.5984364748001099),\n",
       " ('ipad', 0.5951722860336304)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"リクルート\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "無理 だ ー \n",
      "\n",
      "81% 無料 で 遊ん だ ー \n",
      "\n",
      "75% NSTimer が 動か ない 。 なんで だ ー 。 \n",
      "\n",
      "70% だ だ し 、 Template 系 が 入ら ない なぁ 。 \n",
      "\n",
      "69% bitbucket の Slack へ の 連携 が びみょ ー だ \n",
      "\n",
      "68% 迷惑 メール が 来る よう に なっ た \n",
      "\n",
      "67% ちょ ー いい ランチ だっ た ！ たのし ー \n",
      "\n",
      "66% また 、 上 、 ネット つながら ない やつ だ … . \n",
      "\n",
      "66% どうやら 、 ペンギン ちゃん が 今夜 リリース さ れる みたい だ \n",
      "\n",
      "66% 駄目 だ camphchat が 動か ない 。 なんで だ \n",
      "\n",
      "66% すごい シングル トン だ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = \"text_1\"\n",
    "key, index = base.split(\"_\")\n",
    "print(df.ix[int(index)][key])\n",
    "for offset, similality in model.docvecs.most_similar(base):\n",
    "    doctag = model.docvecs.offset2doctag[offset]\n",
    "    key, index = doctag.split(\"_\")\n",
    "    print(\"%s%%\"%int(similality*100),df.ix[int(index)][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import vq, kmeans, whiten\n",
    "centroid, destortion = kmeans(model.docvecs, 10 , iter=100, thresh=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 7 4 1 5]\n",
      "[ 0.28320223  0.34859508  0.2048361   0.29068244  0.28006327]\n"
     ]
    }
   ],
   "source": [
    "labels, dist = vq(model.docvecs, centroid)\n",
    "print(labels[:5]) # どのクラスターに入ってるか\n",
    "print(dist[:5]) # 中心からの距離"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 スタート\n",
      "52% ねむいー\n",
      "63% Doc2Vecの基底ベクトルはWord2Vecのそれと一緒っぽいな\n",
      "66% POSTDのパーカーはポッドキャストに出ればもらえるらしい\n",
      "78% やこちゃんの趣味w\n",
      "\n",
      "\n",
      "1 スタート\n",
      "70% えい、 `from scipy.cluster.vq import vq, kmeans, whiten`\n",
      "82% えっーっと、やこです！！！\n",
      "81% タイガーナッツ気になる\n",
      "73% みんな電子タバコにしてくんないかな\n",
      "\n",
      "\n",
      "2 スタート\n",
      "76% ぶっちゃけ感w\n",
      "73% 日経平均さん！\n",
      "70% 企業のTechBlogで成功しているところってどこやろうなぁ。\n",
      "72% 機械学習で質問できるところないだろうかぁ\n",
      "\n",
      "\n",
      "3 スタート\n",
      "77% フェスや！フェス！\n",
      "69% どういう人をとりたいかでテーマ変わってきそうだから、個人で分散させて、有名人を複数人作るほうが良さそうな気はしている\n",
      "66% 個人でやってるブログが良いことが多いことが多い気がする。それで会社に興味持つ流れのほうが自然だわなぁ、って気はする。\n",
      "69% Theanoはお勉強にはいいけど、めんどくさい。しばらくChainerかなっという感。\n",
      "\n",
      "\n",
      "4 スタート\n",
      "79% 計算中・・・\n",
      "75% 聴き終わった\n",
      "76% フェスは負けまくってる\n",
      "73% 大麦にはグルテンが入ってない！\n",
      "\n",
      "\n",
      "5 スタート\n",
      "71% 無理だー\n",
      "71% k-meansやるぞ\n",
      "72% softmaxを使えば0にはならないからクロスエントロピー大丈夫なのか\n",
      "77% この前見れなかったから楽しみね\n",
      "\n",
      "\n",
      "6 スタート\n",
      "62% おはよー！ 6h 28m ねたよ！ January 27, 2016 at 01:19AM →January 27, 2016 at 08:06AM\n",
      "69% おはよー！ 5h 57m ねたよ！ January 26, 2016 at 01:34AM →January 26, 2016 at 07:46AM\n",
      "70% おはよー！ 8h 31m ねたよ！ January 25, 2016 at 01:09AM →January 25, 2016 at 09:52AM\n",
      "69% おはよー！ 6h 49m ねたよ！ January 23, 2016 at 02:25AM →January 23, 2016 at 09:37AM\n",
      "\n",
      "\n",
      "7 スタート\n",
      "65% 人間よ、24に、分類されよ！\n",
      "69% フェス、カンペキカラダ、勝った！！\n",
      "64% LINEの鍵のやつ、LINEでできるようになったってのもやばいけど、そもそも写真から鍵作れるの、そもそもやばくない？\n",
      "\n",
      "技術的には可能だった、ってことよね\n",
      "57% 芸能ニュースでやばいのがあると、政治的に何かあるのかな、って気になる。\n",
      "\n",
      "3S（Screen, Sports, Sex）を抑えると、政治は上手くいくとかなんとか。\n",
      "\n",
      "\n",
      "8 スタート\n",
      "59% ウィスキー！\n",
      "67% 鍵って、鍵番号だけで複製できるのか。\n",
      "\n",
      "写真から、3Dを再現して、作るんかと思った。\n",
      "69% プロダクションでObjC→Swiftにしたところってあるのかな。\n",
      "70% 企業としては、HackerNews的なキュレーションで良いと思うんだな。\n",
      "\n",
      "今週の社員が、気になった記事みたいなのみたいなまとめとかで。\n",
      "\n",
      "そこに質の高い記事を社員が書いたらピックアップしておく。\n",
      "\n",
      "\n",
      "9 スタート\n",
      "81% あぁ、またフェス行きたい\n",
      "74% あとはOSSの活動報告ぐらいのほうが、良さそう\n",
      "79% Theanoで線形回帰も出来たー！\n",
      "73% これからDeepなやつをやっていこうと思うので、Chainerが一番簡単に書けそう。\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/tweets.csv\")\n",
    "for label in range(10):\n",
    "    print(label,\"スタート\")\n",
    "    print_i = 0\n",
    "    for i in range(model.docvecs.count):\n",
    "        if labels[i] == label:\n",
    "            dist_vec = vq([model.docvecs[i]], centroid)\n",
    "            doctag = model.docvecs.offset2doctag[i]\n",
    "            key, index = doctag.split(\"_\")\n",
    "            if int((1.-dist[i])*100) and not pd.isnull(df.ix[int(index)][key]):\n",
    "#                 print(dist_vec)\n",
    "                print(\"%s%%\"%int((1.-dist[i])*100),df.ix[int(index)][key].strip(\"\\t\\n\"))\n",
    "                print_i +=1\n",
    "                if print_i > 3 : \n",
    "                    print(\"\\n\");break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sim_matrix = []\n",
    "size = model.docvecs.count\n",
    "for i in range(model.docvecs.count):\n",
    "    sim_vec = np.zeros(size)\n",
    "    for word_index, sim_val in model.docvecs.most_similar(i, topn=100):\n",
    "        sim_vec[word_index] = sim_val\n",
    "    sim_matrix.append(sim_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "dimension = 100\n",
    "lsa = TruncatedSVD(dimension)\n",
    "info_matrix = lsa.fit_transform(sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centroid, destortion = kmeans(info_matrix, 10 , iter=100, thresh=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 1 6 5 9]\n",
      "[ 3.56423178  3.75280822  2.52135968  1.90380796  3.18993187]\n"
     ]
    }
   ],
   "source": [
    "labels, dist = vq(info_matrix, centroid)\n",
    "print(labels[:5]) # どのクラスターに入ってるか\n",
    "print(dist[:5]) # 中心からの距離"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 スタート\n",
      "-113%「 個人でやってるブログが良いことが多いことが多い気がする。それで会社に興味持つ流れのほうが自然だわなぁ、って気はする。 」\n",
      "-159%「 という超ど素人の意見です 」\n",
      "-170%「 Mantle to Realmがめんどくさい。結局書かなあかんのか 」\n",
      "-192%「 なんでや 」\n",
      "-260%「 日本のVCでそういうところまでサポートしてくれるところってあるのかな？\n",
      "\n",
      "あんまり聞かない気がするけど 」\n",
      "\n",
      "\n",
      "1 スタート\n",
      "-275%「 人間よ、24に、分類されよ！ 」\n",
      "-277%「 フェス、カンペキカラダ、勝った！！ 」\n",
      "-309%「 BASEで、ケーキ食べて、日本酒飲んで、マジゥクザギャザリングやって、カルカソンヌした 」\n",
      "-277%「 中目黒付近に住んでる人多いのね。びっくりした。 」\n",
      "-182%「 そういえば、僕、チャーシューが、食べれません 」\n",
      "\n",
      "\n",
      "2 スタート\n",
      "-131%「 おはよー！ 6h 28m ねたよ！ January 27, 2016 at 01:19AM →January 27, 2016 at 08:06AM 」\n",
      "-222%「 おはよー！ 5h 57m ねたよ！ January 26, 2016 at 01:34AM →January 26, 2016 at 07:46AM 」\n",
      "-164%「 おはよー！ 6h 49m ねたよ！ January 23, 2016 at 02:25AM →January 23, 2016 at 09:37AM 」\n",
      "-108%「 おはよー！ 7h 53m ねたよ！ January 22, 2016 at 01:10AM →January 22, 2016 at 09:21AM 」\n",
      "-40%「 おはよー！ 5h 58m ねたよ！ January 21, 2016 at 01:38AM →January 21, 2016 at 07:52AM 」\n",
      "\n",
      "\n",
      "3 スタート\n",
      "-196%「 体をアルカリ性に変えてこ 」\n",
      "-237%「 2こ同じビルド番号であがってもうた 」\n",
      "-144%「 クライアント側実装していて、サーバー開発の時に、swagger入れておいて正解だったと思ってる 」\n",
      "-160%「 集中力を鍛える 」\n",
      "-147%「 ダイオウイカ対スーパーショット 」\n",
      "\n",
      "\n",
      "4 スタート\n",
      "-156%「 鍵って、鍵番号だけで複製できるのか。\n",
      "\n",
      "写真から、3Dを再現して、作るんかと思った。 」\n",
      "-298%「 Sumally Pocketって、Box me upやん！！ 」\n",
      "-124%「 BASEで朝活。ラジオ体操した。 」\n",
      "-168%「 中目黒に京野菜のここらやがある！！！ 」\n",
      "-222%「 ヨガろう。戦士のポーズ！！ 」\n",
      "\n",
      "\n",
      "5 スタート\n",
      "-90%「 えい、 `from scipy.cluster.vq import vq, kmeans, whiten` 」\n",
      "-151%「 POSTDのパーカーはポッドキャストに出ればもらえるらしい 」\n",
      "-155%「 ぶっちゃけ感w 」\n",
      "-137%「 えっーっと、やこです！！！ 」\n",
      "-165%「 おはよー！ 8h 31m ねたよ！ January 25, 2016 at 01:09AM →January 25, 2016 at 09:52AM 」\n",
      "\n",
      "\n",
      "6 スタート\n",
      "-152%「 計算中・・・ 」\n",
      "-98%「 聴き終わった 」\n",
      "-102%「 フェスは負けまくってる 」\n",
      "-144%「 クラスメソッドのブログはすごい気がする。真っ先に試してみました系が多くて 」\n",
      "-251%「 チャリに乗ってるうめちゃんに出会った 」\n",
      "\n",
      "\n",
      "7 スタート\n",
      "-212%「 あぁ、またフェス行きたい 」\n",
      "-158%「 あとはOSSの活動報告ぐらいのほうが、良さそう 」\n",
      "-248%「 Theanoで線形回帰も出来たー！ 」\n",
      "-158%「 グルテン断ちとか難しすぎるぞ 」\n",
      "-212%「 togglのベストプラクティスを知りたい 」\n",
      "\n",
      "\n",
      "8 スタート\n",
      "-365%「 ねむいー 」\n",
      "-207%「 Doc2Vecの基底ベクトルはWord2Vecのそれと一緒っぽいな 」\n",
      "-215%「 やこちゃんの趣味w 」\n",
      "-150%「 やこちゃんのMCやばいw 」\n",
      "-189%「 休学ってカラオケ形式だったのか 」\n",
      "\n",
      "\n",
      "9 スタート\n",
      "-256%「 無理だー 」\n",
      "-218%「 k-meansやるぞ 」\n",
      "-164%「 softmaxを使えば0にはならないからクロスエントロピー大丈夫なのか 」\n",
      "-187%「 この前見れなかったから楽しみね 」\n",
      "-167%「 Chainerちょっと癖あるなぁ 」\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/tweets.csv\")\n",
    "for label in range(10):\n",
    "    print(label,\"スタート\")\n",
    "    print_i = 0\n",
    "    for i in range(model.docvecs.count):\n",
    "        if labels[i] == label:\n",
    "            doctag = model.docvecs.offset2doctag[i]\n",
    "            key, index = doctag.split(\"_\")\n",
    "            if int((1.-dist[i])*100) and not pd.isnull(df.ix[int(index)][key]):\n",
    "#                 print(dist_vec)\n",
    "                print(\"%s%%「\"%int((1.-dist[i])*100),df.ix[int(index)][key].strip(\"\\t\\n\"),\"」\")\n",
    "                print_i +=1\n",
    "                if print_i > 4 : break;\n",
    "    print(\"\\n\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
