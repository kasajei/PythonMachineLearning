{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chainerで線形回帰してみる\n",
    "\n",
    "線形回帰についてはtensorflowを参考。ここではChainer独自のところを説明していく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, Variable, optimizers, serializers, utils\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChainerでもVariableを使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<var@11137b6d8>\n",
      "x.data -> [ 2.]\n",
      "x.grad -> None\n",
      "y.data-> [ 4.]\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([2], dtype=np.float32)\n",
    "x = Variable(x_data)\n",
    "print(x)\n",
    "print(\"x.data ->\", x.data)\n",
    "print(\"x.grad ->\", x.grad) # この段階でgradはNone\n",
    "y = x ** 2\n",
    "print(\"y.data->\", y.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アウトプットに対して、`backward()`とすると、その関数を作っているVariableでのgradの値が入力される。この場合はxでの偏微分の値。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.]\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(x.grad)\n",
    "y.zerograd() # 偏微分の値は蓄積されていくので、zerogradで初期化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chainerが面白いのは、F.LinerやL.Linerが\n",
    "$$ \\vec{y} = W\\vec{x} +\\vec{b}$$\n",
    "の形を持った関数で、$W$や$\\vec{b}$はランダムに初期化してくれている。\n",
    "\n",
    "`F.Liner(1,1)`だと、1次元→1次元で、あとでMNISTなどで使う784次元→10次元であれば`F.Liner(784,10)`とすれば、&W&は784x10の行列で、$\\vec{b}\\in\\mathbb{R}^{10}$となるように作ってくれる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.33073533]]\n",
      "[ 0.]\n"
     ]
    }
   ],
   "source": [
    "f = F.Linear(1,1) # 1次元→1次元の y = Wx + b\n",
    "print(f.W.data)\n",
    "print(f.b.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100個のデータセットを作って、コスト関数を平均二乗誤差関数で定義してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.27102425694465637, dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = np.random.rand(100,1).astype(\"float32\")\n",
    "y_data = x_data * 0.1 + 0.3\n",
    "x = Variable(x_data)\n",
    "y = Variable(y_data)\n",
    "loss = F.mean_squared_error(f(x), y) # 平均二乗誤差\n",
    "loss.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチで一気に計算すると器用に偏微分の値は蓄積されていくみたいなので、偏微分を計算する前に初期化しておくための`zerograds`が準備されているので呼ぶ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.62662262]]\n",
      "[ 0.7638604]\n"
     ]
    }
   ],
   "source": [
    "f.zerograds() # 微分値の初期化\n",
    "loss.backward() # それぞれの偏微分を計算する\n",
    "print(f.W.grad) # fのWでの変微分の値が求まる\n",
    "print(f.b.grad) # fのbでの変微分の値が求まる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コスト関数の最小化の手法はoptimizersに準備されている。最急降下法がなさそうなので、確率的最急降下法を使う。`setup`で変数の更新対象の関数をセットする。そして、`update`でパラメーターを更新してくれる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.01742399]]\n",
      "[-0.3819302]\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.SGD(0.5) # 確率的最急降下法\n",
    "optimizer.setup(f)\n",
    "optimizer.update()\n",
    "print(f.W.data)\n",
    "print(f.b.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記を合わせてイテレーションして学習を進めてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[ 1.03781068]] [-0.20832953]\n",
      "20 [[ 0.35380483]] [ 0.15003617]\n",
      "40 [[ 0.17018947]] [ 0.25852767]\n",
      "60 [[ 0.11941081]] [ 0.28853089]\n",
      "80 [[ 0.10536806]] [ 0.29682824]\n",
      "100 [[ 0.10148452]] [ 0.29912287]\n",
      "120 [[ 0.10041054]] [ 0.29975745]\n",
      "140 [[ 0.10011356]] [ 0.29993293]\n",
      "160 [[ 0.10003141]] [ 0.29998147]\n",
      "180 [[ 0.10000867]] [ 0.29999489]\n",
      "200 [[ 0.10000242]] [ 0.29999858]\n"
     ]
    }
   ],
   "source": [
    "for step in range(201):\n",
    "    loss = F.mean_squared_error(f(x), y)  # 再度コスト関数を計算する\n",
    "    f.zerograds() # gradは保存されていくので初期化する\n",
    "    loss.backward() # 微分を計算\n",
    "    optimizer.update() # 確率的勾配降下法で fをupdateする\n",
    "    if step % 20 == 0:\n",
    "        print(step,f.W.data, f.b.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "綺麗に、学習されている。\n",
    "\n",
    "次に、Chainというクラスを使うことにより、もう少し便利に、上記が出来ることを見ていく。深層学習などの複雑なニューラルネットワークを生成するには、こちらを使うほうが便利そうだ。やっていることは上記と同じなので、説明は省く。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LinearRegression(Chain):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__(\n",
    "            l1 = L.Linear(1,1)\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x, y):\n",
    "        return F.mean_squared_error(self.l1(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "optimizer = optimizers.SGD(0.5)\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data ->  [[ 0.36174315]\n",
      " [ 0.45916244]\n",
      " [ 0.55743873]\n",
      " [ 0.20309481]\n",
      " [ 0.90550619]]\n",
      "y_data ->  [[ 0.33617434]\n",
      " [ 0.34591627]\n",
      " [ 0.35574389]\n",
      " [ 0.32030949]\n",
      " [ 0.39055064]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_data = np.random.rand(100,1).astype(\"float32\")\n",
    "y_data = x_data * 0.1 + 0.3\n",
    "print(\"x_data -> \", x_data[:5])\n",
    "print(\"y_data -> \", y_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[ 0.7579459]] [-0.05036949]\n",
      "20 [[ 0.28956166]] [ 0.20411299]\n",
      "40 [[ 0.15403411]] [ 0.27266765]\n",
      "60 [[ 0.1154023]] [ 0.29220897]\n",
      "80 [[ 0.10439041]] [ 0.2977792]\n",
      "100 [[ 0.10125148]] [ 0.29936698]\n",
      "120 [[ 0.10035673]] [ 0.29981956]\n",
      "140 [[ 0.10010167]] [ 0.29994857]\n",
      "160 [[ 0.10002899]] [ 0.29998535]\n",
      "180 [[ 0.10000826]] [ 0.29999584]\n",
      "200 [[ 0.10000234]] [ 0.29999882]\n"
     ]
    }
   ],
   "source": [
    "for step in range(201):\n",
    "    train_x = Variable(x_data)\n",
    "    train_y = Variable(y_data)\n",
    "    optimizer.update(model, train_x, train_y)\n",
    "    if step % 20 == 0:\n",
    "        print(step, model.l1.W.data, model.l1.b.data) \n",
    "    \n",
    "#     model.zerograds()\n",
    "#     loss = model(train_x, train_y)\n",
    "#     loss.backword()\n",
    "#     optimizer.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "綺麗に学習できている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MNISTをパーセプトロンで学習してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000 10000\n",
      "784\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "28x28の画像のモノクロ(白0→黒1)が１次元配列で入っている [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "最初のラベルは 5\n",
      "[5 0 4 ..., 8 4 8]\n"
     ]
    }
   ],
   "source": [
    "import pickle, gzip\n",
    "f = gzip.open('../data/mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "print(len(train_set[0]), len(valid_set[0]), len(test_set[0]))\n",
    "train_set_x, train_set_y  = train_set\n",
    "test_set_x, test_set_y = test_set\n",
    "print(len(train_set_x[0]))\n",
    "print(train_set_x)\n",
    "print(\"28x28の画像のモノクロ(白0→黒1)が１次元配列で入っている\",train_set_x[:5])\n",
    "print(\"最初のラベルは\",train_set_y[0])\n",
    "\n",
    "# あとで、softmax_cross_entropyを使うときに型の判定があり、np.int32じゃないといけない\n",
    "train_set_y = train_set_y.astype(np.int32)\n",
    "test_set_y = test_set_y.astype(np.int32)\n",
    "print(train_set_y )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MNISTPerceptronClassifier(Chain):\n",
    "    def __init__(self):\n",
    "        super(MNISTPerceptronClassifier, self).__init__(\n",
    "            l1 = L.Linear(784, 10)\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x, y):\n",
    "        pridict = self.l1(x)\n",
    "        self.loss = F.softmax_cross_entropy(pridict, y)\n",
    "        self.accuracy = F.accuracy(pridict, y) \n",
    "        return self.loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = MNISTPerceptronClassifier()\n",
    "optimizer = optimizers.SGD(0.1) # 確率的勾配降下法なので、ちょっと大きめに\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "for step in range(1000):\n",
    "    batch_index = np.random.randint(len(train_set[0])-batch_size)\n",
    "    batch_x = Variable(train_set_x[batch_index:batch_index+batch_size])\n",
    "    batch_y = Variable(train_set_y[batch_index:batch_index+batch_size])\n",
    "    optimizer.update(model, batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.907800018787384\n"
     ]
    }
   ],
   "source": [
    "model(Variable(test_set_x),Variable(test_set_y))\n",
    "print(model.accuracy.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90%ぐらいの制度がでる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式のサンプルを見ると、Chainでは、ネットワークの構成だけにしておいて、L.Classifierを使うのが便利そう。[ドキュメント](http://docs.chainer.org/en/stable/_modules/chainer/links/model/classifier.html)を見ると、ロス関数としてソフトマックス関数が設定されていたり、`self.accuracy`も設定されているので、便利に使える。イニシャライズするときに、変数で渡せば任意のものが使える。分類をするときは最後にこれをつければ良さそう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9075000286102295\n"
     ]
    }
   ],
   "source": [
    "class MNISTPerceptron(Chain):\n",
    "    def __init__(self):\n",
    "        super(MNISTPerceptron, self).__init__(\n",
    "            l1 = L.Linear(784, 10)\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.l1(x)\n",
    "    \n",
    "model = L.Classifier(MNISTPerceptron())\n",
    "optimizer.setup(model)\n",
    "batch_size = 100\n",
    "for step in range(1000):\n",
    "    batch_index = np.random.randint(len(train_set[0])-batch_size)\n",
    "    batch_x = Variable(train_set_x[batch_index:batch_index+batch_size])\n",
    "    batch_y = Variable(train_set_y[batch_index:batch_index+batch_size])\n",
    "    optimizer.update(model, batch_x, batch_y)\n",
    "    \n",
    "model(Variable(test_set_x),Variable(test_set_y))\n",
    "print(model.accuracy.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
