{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chainerで線形回帰してみる\n",
    "\n",
    "線形回帰についてはtensorflowを参考。ここではChainer独自のところを説明していく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, Variable, optimizers, serializers, utils\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChainerでもVariableを使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<var@10cd02a58>\n",
      "x.data -> [ 2.]\n",
      "x.grad -> None\n",
      "y.data-> [ 4.]\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([2], dtype=np.float32)\n",
    "x = Variable(x_data)\n",
    "print(x)\n",
    "print(\"x.data ->\", x.data)\n",
    "print(\"x.grad ->\", x.grad) # この段階でgradはNone\n",
    "y = x ** 2\n",
    "print(\"y.data->\", y.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アウトプットに対して、`backward()`とすると、その関数を作っているVariableでのgradの値が入力される。この場合はxでの偏微分の値。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.]\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(x.grad)\n",
    "y.zerograd() # 偏微分の値は蓄積されていくので、zerogradで初期化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chainerが面白いのは、F.LinerやL.Linerが\n",
    "$$ \\vec{y} = W\\vec{x} +\\vec{b}$$\n",
    "の形を持った関数で、$W$や$\\vec{b}$はランダムに初期化してくれている。\n",
    "\n",
    "`F.Liner(1,1)`だと、1次元→1次元で、あとでMNISTなどで使う784次元→10次元であれば`F.Liner(784,10)`とすれば、&W&は784x10の行列で、$\\vec{b}\\in\\mathbb{R}^{10}$となるように作ってくれる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.07693613]]\n",
      "[ 0.]\n"
     ]
    }
   ],
   "source": [
    "f = F.Linear(1,1) # 1次元→1次元の y = Wx + b\n",
    "print(f.W.data)\n",
    "print(f.b.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100個のデータセットを作って、コスト関数を平均二乗誤差関数で定義してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.11790099740028381, dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = np.random.rand(100,1).astype(\"float32\")\n",
    "y_data = x_data * 0.1 + 0.3\n",
    "x = Variable(x_data)\n",
    "y = Variable(y_data)\n",
    "loss = F.mean_squared_error(f(x), y) # 平均二乗誤差\n",
    "loss.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチで一気に計算すると器用に偏微分の値は蓄積されていくみたいなので、偏微分を計算する前に初期化しておくための`zerograds`が準備されているので呼ぶ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.36616719]]\n",
      "[ 0.40639976]\n"
     ]
    }
   ],
   "source": [
    "f.zerograds() # 微分値の初期化\n",
    "loss.backward() # それぞれの偏微分を計算する\n",
    "print(f.W.grad) # fのWでの変微分の値が求まる\n",
    "print(f.b.grad) # fのbでの変微分の値が求まる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コスト関数の最小化の手法はoptimizersに準備されている。最急降下法がなさそうなので、確率的最急降下法を使う。`setup`で変数の更新対象の関数をセットする。そして、`update`でパラメーターを更新してくれる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.89385253]]\n",
      "[-0.20319988]\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.SGD(0.5) # 確率的最急降下法\n",
    "optimizer.setup(f)\n",
    "optimizer.update()\n",
    "print(f.W.data)\n",
    "print(f.b.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記を合わせてイテレーションして学習を進めてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[ 0.8787027]] [-0.10889729]\n",
      "20 [[ 0.31169686]] [ 0.18368191]\n",
      "40 [[ 0.15814929]] [ 0.26804954]\n",
      "60 [[ 0.11597254]] [ 0.29122379]\n",
      "80 [[ 0.10438737]] [ 0.29758933]\n",
      "100 [[ 0.10120513]] [ 0.29933783]\n",
      "120 [[ 0.10033102]] [ 0.29981813]\n",
      "140 [[ 0.10009093]] [ 0.29995003]\n",
      "160 [[ 0.10002498]] [ 0.2999863]\n",
      "180 [[ 0.10000686]] [ 0.29999626]\n",
      "200 [[ 0.10000188]] [ 0.29999897]\n"
     ]
    }
   ],
   "source": [
    "for step in range(201):\n",
    "    loss = F.mean_squared_error(f(x), y)  # 再度コスト関数を計算する\n",
    "    f.zerograds() # gradは保存されていくので初期化する\n",
    "    loss.backward() # 微分を計算\n",
    "    optimizer.update() # 確率的勾配降下法で fをupdateする\n",
    "    if step % 20 == 0:\n",
    "        print(step,f.W.data, f.b.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "綺麗に、学習されている。\n",
    "\n",
    "次に、Chainというクラスを使うことにより、もう少し便利に、上記が出来ることを見ていく。深層学習などの複雑なニューラルネットワークを生成するには、こちらを使うほうが便利そうだ。やっていることは上記と同じなので、説明は省く。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LinearRegression(Chain):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__(\n",
    "            l1 = L.Linear(1,1)\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x, y):\n",
    "        return F.mean_squared_error(self.l1(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "optimizer = optimizers.SGD(0.5)\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data ->  [[ 0.81184357]\n",
      " [ 0.82798856]\n",
      " [ 0.54457718]\n",
      " [ 0.36221436]\n",
      " [ 0.92550302]]\n",
      "y_data ->  [[ 0.38118437]\n",
      " [ 0.38279888]\n",
      " [ 0.35445774]\n",
      " [ 0.33622146]\n",
      " [ 0.39255032]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_data = np.random.rand(100,1).astype(\"float32\")\n",
    "y_data = x_data * 0.1 + 0.3\n",
    "print(\"x_data -> \", x_data[:5])\n",
    "print(\"y_data -> \", y_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[-0.46129084]] [ 0.93413109]\n",
      "20 [[-0.08264302]] [ 0.4060303]\n",
      "40 [[ 0.05201237]] [ 0.32785842]\n",
      "60 [[ 0.08739172]] [ 0.30731952]\n",
      "80 [[ 0.09668729]] [ 0.30192316]\n",
      "100 [[ 0.09912962]] [ 0.30050528]\n",
      "120 [[ 0.09977131]] [ 0.30013278]\n",
      "140 [[ 0.09993992]] [ 0.30003488]\n",
      "160 [[ 0.09998421]] [ 0.30000919]\n",
      "180 [[ 0.09999584]] [ 0.30000243]\n",
      "200 [[ 0.09999894]] [ 0.30000064]\n"
     ]
    }
   ],
   "source": [
    "for step in range(201):\n",
    "    train_x = Variable(x_data)\n",
    "    train_y = Variable(y_data)\n",
    "    optimizer.update(model, train_x, train_y)\n",
    "    if step % 20 == 0:\n",
    "        print(step, model.l1.W.data, model.l1.b.data) \n",
    "    \n",
    "#     model.zerograds()\n",
    "#     loss = model(train_x, train_y)\n",
    "#     loss.backword()\n",
    "#     optimizer.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "綺麗に学習できている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MNISTをパーセプトロンで学習してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000 10000\n",
      "784\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "28x28の画像のモノクロ(白0→黒1)が１次元配列で入っている [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "最初のラベルは 5\n",
      "[5 0 4 ..., 8 4 8]\n"
     ]
    }
   ],
   "source": [
    "import pickle, gzip\n",
    "f = gzip.open('data/mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "print(len(train_set[0]), len(valid_set[0]), len(test_set[0]))\n",
    "train_set_x, train_set_y  = train_set\n",
    "test_set_x, test_set_y = test_set\n",
    "print(len(train_set_x[0]))\n",
    "print(train_set_x)\n",
    "print(\"28x28の画像のモノクロ(白0→黒1)が１次元配列で入っている\",train_set_x[:5])\n",
    "print(\"最初のラベルは\",train_set_y[0])\n",
    "\n",
    "# あとで、softmax_cross_entropyを使うときに型の判定があり、np.int32じゃないといけない\n",
    "train_set_y = train_set_y.astype(np.int32)\n",
    "test_set_y = test_set_y.astype(np.int32)\n",
    "print(train_set_y )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MNISTPerseptron(Chain):\n",
    "    def __init__(self):\n",
    "        super(MNISTPerseptron, self).__init__(\n",
    "            l1 = L.Linear(784, 10)\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x, y):\n",
    "        pridict = self.l1(x)\n",
    "        self.loss = F.softmax_cross_entropy(pridict, y)\n",
    "        self.accuracy = F.accuracy(pridict, y) \n",
    "        return self.loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = MNISTPerseptron()\n",
    "optimizer = optimizers.SGD(0.1) # 確率的勾配降下法なので、ちょっと大きめに\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "for step in range(1000):\n",
    "    batch_index = np.random.randint(len(train_set[0])-batch_size)\n",
    "    batch_x = Variable(train_set_x[batch_index:batch_index+batch_size])\n",
    "    batch_y = Variable(train_set_y[batch_index:batch_index+batch_size])\n",
    "    optimizer.update(model, batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9071999788284302\n"
     ]
    }
   ],
   "source": [
    "model(Variable(test_set_x),Variable(test_set_y))\n",
    "print(model.accuracy.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90%ぐらいの制度がでる"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
